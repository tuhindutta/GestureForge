# ğŸ–ï¸ GestureForge

Endâ€‘toâ€‘end toolkit for collecting handâ€‘gesture landmark data, training gestureâ€‘recognition models, and exporting readyâ€‘toâ€‘deploy ONNX weights.

## ğŸ“Œ Overview
GestureForge lets you quickly record palm/arm landmarks (image or video), train a model (Randomâ€‘Forest for images, GRU for sequences), sanityâ€‘check the result, ready to export portable ONNX weights for C++ / Web / Edge deployment.

## ğŸ“‚ Directory Layout
```yaml
GestureForge/
â”‚   .gitignore
â”‚   LICENSE                                  # MITÂ orÂ ApacheÂ 2.0 (your pick)
â”‚   README.md                                # short project summary
â”‚   requirements.txt                         # exact library versions
â”‚
â”œâ”€â”€ ImageDataGenerator_Training_Inference/
â”‚   â”œâ”€â”€ data_collect.py                      # record singleâ€‘frame samples
â”‚   â”œâ”€â”€ train.py                             # RandomForest pipeline
â”‚   â”œâ”€â”€ inference.py                         # quick accuracy sanityâ€‘check
â”‚   â”œâ”€â”€ train_config.yaml                    # (unused â€” RF uses defaults)
â”‚   â”œâ”€â”€ outputs/                             # pickles + encoders after each run
â”‚   â””â”€â”€ utils/                               # utilities
â”‚       â”œâ”€â”€ arm_detect.py                    # arm_detect
â”‚       â”œâ”€â”€ hand_detect.py                   # hand_detect
â”‚       â”œâ”€â”€ palm_detect.py                   # palm_detect
â”‚       â””â”€â”€ pose_landmarker_heavy.task       # pose_landmarker_model
â”‚
â””â”€â”€ VideoDataGenerator_Training_Inference/
    â”œâ”€â”€ data_collect.py                      # record multiâ€‘frame sequences
    â”œâ”€â”€ train.py                             # GRU training driven by YAML
    â”œâ”€â”€ inference.py                         # sanityâ€‘check sequence model
    â”œâ”€â”€ train_config.yaml                    # epochs, hidden_size, etc.
    â”œâ”€â”€ outputs/                             # *.pt, pickles, CSV metadata
    â””â”€â”€ utils/                               # same detector modules as above
        â”œâ”€â”€ arm_detect.py                    # arm_detect
        â”œâ”€â”€ hand_detect.py                   # hand_detect
        â”œâ”€â”€ palm_detect.py                   # palm_detect
        â””â”€â”€ trainer.py                       # trainer_classes
```

## âš™ï¸ Prerequisites
#### 1ï¸âƒ£ PythonÂ 3.11 virtualâ€‘env
```bash
python3.11 -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
```
source .venv/bin/activate   # Windows: .venv\Scripts\activate

#### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

## ğŸš€ QuickÂ Start
#### 1ï¸âƒ£ Imageâ€‘Gesture Workflow (single frame)
```bash
cd ImageDataGenerator_Training_Inference

# (1) Record N samples of label "wave" using only palm landmarks
python data_collect.py okay -r

# (2) Train Randomâ€‘Forest (defaults)
python train.py

# (3) Sanityâ€‘check inference
python inference.py -ant
```

#### 2ï¸âƒ£ Videoâ€‘Gesture Workflow (frame sequences)
```bash
cd VideoDataGenerator_Training_Inference

# (1) Record sequences (palmÂ + arm landmarks, 60Â frames each)
python data_collect.py clap -r

# (2) Configure training
vim train_config.yaml

# (3) Train GRU
python train.py         # writes model.pt & label_encoder.pkl

# (4) Sanityâ€‘check inference
python inference.py
```

## ğŸ”„ LandmarkÂ Pipeline
```txt
MediaPipe (raw landmarks)
   â””â”€ palm_detect  â†’  palm tensor  (scaled + normalised)
   â””â”€ arm_detect   â†’  arm  tensor  (same normalisation)
      â””â”€ hand_detect (fusion + relative geometry)
            â†’ final hand vector      #  used by trainer / inference
```
#### Frame acceptance rules configured per run:
- By default, palms are always detected unless stated otherwise.
- Detects only Arms.
- Detects Arms along with Palms.

## ğŸ§  Training Details
#### Randomâ€‘Forest (Images)
- `sklearn.ensemble.RandomForestClassifier` with default hyperâ€‘params
- Works well because static landmark vectors are highly separable across gestures.

#### GRU (Video)
- Input shape `(batch, seq_len, features)`
- Configurable via `train_config.yaml`:
  ```yaml
  data:
  batch_size: 3
  model:
    gru_hidden_size: 32
    gru_num_layers: 2
  training:
    epochs: 20
    learning_rate: 1e-3
    early_stopping_accuracy_thresh: 0.05
    early_stopping_toll: 4
  ```

## ğŸ” Model Quality Check
Both pipelines include an `inference.py` script that reproduces preprocessing and runs the freshlyâ€‘trained model to ensure data & training are correct before downstream deployment.

## ğŸ¤ Contributing
Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes. For more details and updates, visit the [GitHub Repository](https://github.com/tuhindutta/GestureForge).

## ğŸ™ Acknowledgements
- Google MediaPipe team for awesome realâ€‘time landmark tracking
- PyTorch & scikitâ€‘learn communities for the ML backbone
