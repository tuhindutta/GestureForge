# ğŸ–ï¸ GestureForge

Endâ€‘toâ€‘end toolkit for collecting handâ€‘gesture landmark data, training gestureâ€‘recognition models, and exporting readyâ€‘toâ€‘deploy ONNX weights.

## ğŸ“Œ Overview
GestureForge lets you quickly record palm/arm landmarks (static and dynamic gestures), train a model (Randomâ€‘Forest for static, GRU for dynamic gestures), sanityâ€‘check the result, and use the trained model for export as portable ONNX weights, enabling deployment across C++, web, and edge environments.

## ğŸ“‚ Directory Layout
```yaml
GestureForge/
â”‚   .gitignore
â”‚   LICENSE                                  # MITÂ orÂ ApacheÂ 2.0 (your pick)
â”‚   README.md                                # short project summary
â”‚   requirements.txt                         # exact library versions
â”‚
â”œâ”€â”€ ImageDataGenerator_Training_Inference/
â”‚   â”œâ”€â”€ data_collect.py                      # record singleâ€‘frame samples
â”‚   â”œâ”€â”€ train.py                             # RandomForest pipeline
â”‚   â”œâ”€â”€ inference.py                         # quick accuracy sanityâ€‘check
â”‚   â”œâ”€â”€ outputs/                             # pickles + encoders after each run
â”‚   â””â”€â”€ utils/                               # utilities
â”‚       â”œâ”€â”€ arm_detect.py                    # arm_detect
â”‚       â”œâ”€â”€ hand_detect.py                   # hand_detect
â”‚       â”œâ”€â”€ palm_detect.py                   # palm_detect
â”‚       â””â”€â”€ pose_landmarker_heavy.task       # pose_landmarker_model
â”‚
â””â”€â”€ VideoDataGenerator_Training_Inference/
    â”œâ”€â”€ data_collect.py                      # record multiâ€‘frame sequences
    â”œâ”€â”€ train.py                             # GRU training driven by YAML
    â”œâ”€â”€ inference.py                         # sanityâ€‘check sequence model
    â”œâ”€â”€ train_config.yaml                    # epochs, hidden_size, etc.
    â”œâ”€â”€ outputs/                             # *.pt, pickles, CSV metadata
    â””â”€â”€ utils/                               # same detector modules as above
        â”œâ”€â”€ arm_detect.py                    # arm_detect
        â”œâ”€â”€ hand_detect.py                   # hand_detect
        â”œâ”€â”€ palm_detect.py                   # palm_detect
        â”œâ”€â”€ trainer.py                       # trainer_classes
        â””â”€â”€ pose_landmarker_heavy.task 
```

## âš™ï¸ Prerequisites
#### 1ï¸âƒ£ PythonÂ 3.11 virtualâ€‘env
```bash
python3.11 -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
```
source .venv/bin/activate   # Windows: .venv\Scripts\activate

#### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

## ğŸš€ QuickÂ Start
#### 1ï¸âƒ£ Imageâ€‘Gesture Workflow (single frame)
```bash
cd ImageDataGenerator_Training_Inference

# (1) Record N samples of label "wave" using only palm landmarks
python data_collect.py okay -r

# (2) Train Randomâ€‘Forest (defaults)
python train.py

# (3) Sanityâ€‘check inference
python inference.py -ant
```

#### 2ï¸âƒ£ Videoâ€‘Gesture Workflow (frame sequences)
```bash
cd VideoDataGenerator_Training_Inference

# (1) Record sequences (palmÂ + arm landmarks, 60Â frames each)
python data_collect.py clap -r

# (2) Configure training
vim train_config.yaml

# (3) Train GRU
python train.py         # writes model.pt & label_encoder.pkl

# (4) Sanityâ€‘check inference
python inference.py
```

## ğŸ”„ LandmarkÂ Pipeline
```txt
MediaPipe (raw landmarks)
   â””â”€ palm_detect  â†’  palm tensor  (scaled + normalised)
   â””â”€ arm_detect   â†’  arm  tensor  (same normalisation)
      â””â”€ hand_detect (fusion + relative geometry)
            â†’ final hand vector      #  used by trainer / inference
```
#### Frame acceptance rules configured per run:
- By default, palms are always detected unless stated otherwise. (_Either detect single or both palms_)
- Detects only Arms. (_Either detect single or both arms_)
- Detects Arms along with Palms.

## ğŸ§  Training Details
#### Randomâ€‘Forest (Images)
- `sklearn.ensemble.RandomForestClassifier` with default hyperâ€‘params
- Works well because static landmark vectors are highly separable across gestures.

#### GRU (Video)
- Input shape `(batch, seq_len, features)`
- Configurable via `train_config.yaml`:
  ```yaml
    data:
      batch_size: 2
    model:
      bidirectional_gru: false
      gru_hidden_size: 32
      gru_num_layers: 2
    training:
      epochs: 100
      learning_rate: 1e-3
      early_stopping_accuracy_thresh: false
      early_stopping_toll: 4
  ```

## ğŸ” Model Quality Check
Both pipelines include an `inference.py` script that reproduces preprocessing and runs the freshlyâ€‘trained model to ensure data & training are correct before downstream deployment.

## ğŸ¤ Contributing
Contributions are welcome! Please fork the repository and submit a pull request for any enhancements or bug fixes. For more details and updates, visit the [GitHub Repository](https://github.com/tuhindutta/GestureForge).

## ğŸ™ Acknowledgements
- Google MediaPipe team for awesome realâ€‘time landmark tracking
- PyTorch & scikitâ€‘learn communities for the ML backbone

## ğŸ”® Future Prospects
GestureForge is built as a modular utility framework to streamline the process of recording, training, and validating hand gesture recognition models using landmark data. While the current implementation focuses on preparing models for accuracy verification and proof-of-concept testing, it lays the foundation for several impactful extensions in the future:
- ğŸš€ Quick Training for Real-world Applications
  - Enables rapid training of lightweight models (like RandomForest or GRU) on custom gestures, tailored to specific domains.
  - Helps developers and researchers prototype gesture-controlled interfaces without deep ML expertise.
- ğŸ“± Edge & Embedded System Deployment
  - With lightweight models and structured landmark-based input, it becomes feasible to deploy trained models on microcontrollers, Raspberry Pi, or edge AI devices.
  - This opens doors to offline gesture-controlled IoT applications without reliance on cloud processing.
- ğŸŒ Integration with Internal Networks & Smart Environments
  - Can be embedded in internal enterprise systems to control operations via hand gestures â€” ideal for touchless interaction in secure or sterile environments.
  - Extendable to internal automation tools, remote devices, or closed-network industrial use-cases using IoT hubs.
- ğŸ§  Emergency Gesture Detection via CCTV Integration
  - GestureForge can power gesture-based emergency response systems using existing CCTV infrastructure.
  - Trained models can be embedded into surveillance workflows to allow people to silently signal distress or request help by making recognizable gestures in view of a camera.
  - This enables automated invocation of emergency protocols in public places like train stations, hospitals, campuses, or offices, thus providing an inclusive, silent, and accessible form of communication in critical moments.
- ğŸ–¥ï¸ Cross-platform Support
  - Future ONNX export support (under development) allows integration with C++ or JavaScript runtimes for native desktop or web-based gesture controls.
- ğŸ“¦ Custom Dataset & Gesture Expansion
  - Offers flexibility to continually update gesture datasets, enabling adaptation for different languages, sign systems, or domain-specific controls.
- ğŸ§© Plugin-based Architecture Possibility
  - Can evolve into a plugin-ready ecosystem where developers can swap detection logic, models, and inference layers based on the use case (e.g., robotics, home automation, AR/VR).
